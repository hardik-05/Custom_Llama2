{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d754b7bb47ad4654884b926d8ed21c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fc7e5606e834b5db0698afa0c3ff429",
              "IPY_MODEL_c451b773fed748da8a8884f0f91e06c3",
              "IPY_MODEL_354e9dac67c34f379595dc3fe33995c1"
            ],
            "layout": "IPY_MODEL_ec151cdae2e64aca93cd292f92810fdf"
          }
        },
        "9fc7e5606e834b5db0698afa0c3ff429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e559a573a7e40cba38ce90feb3ae269",
            "placeholder": "​",
            "style": "IPY_MODEL_a018912e6277494faffbbecc208d5b64",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c451b773fed748da8a8884f0f91e06c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d35ab40fe6c4704b310146bd0412bba",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffcfadd2fa954474a37a67ab284d642b",
            "value": 2
          }
        },
        "354e9dac67c34f379595dc3fe33995c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68563c45857f42bd93a7845001693cf1",
            "placeholder": "​",
            "style": "IPY_MODEL_71afa117ae664d29b92a74866c3358b7",
            "value": " 2/2 [01:04&lt;00:00, 29.33s/it]"
          }
        },
        "ec151cdae2e64aca93cd292f92810fdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e559a573a7e40cba38ce90feb3ae269": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a018912e6277494faffbbecc208d5b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d35ab40fe6c4704b310146bd0412bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffcfadd2fa954474a37a67ab284d642b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68563c45857f42bd93a7845001693cf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71afa117ae664d29b92a74866c3358b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7204c1dbbd984a1a842dc24b4427608e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6708e280be53489a8b4e2f5c7683968c",
              "IPY_MODEL_b9996ecfbc584bd3a3eb30b34a3d6f12",
              "IPY_MODEL_6d19d7b7d0bb455f8d80133ced67da4c"
            ],
            "layout": "IPY_MODEL_e9a3d8bb9fa542f98660f1bc93fc3f1d"
          }
        },
        "6708e280be53489a8b4e2f5c7683968c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e5b6c140d47403eba82aa0afa1992db",
            "placeholder": "​",
            "style": "IPY_MODEL_d3168e27333d435f9ccb9a3bd966887f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b9996ecfbc584bd3a3eb30b34a3d6f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c4f40497c364181b9cdc61b227c75d4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4e2fd34f52346fc90319b3183d0704b",
            "value": 2
          }
        },
        "6d19d7b7d0bb455f8d80133ced67da4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_becab5a8ce0f46279577259e5e1b523e",
            "placeholder": "​",
            "style": "IPY_MODEL_2cfd3470fd734418b393a4f2b7de5f0d",
            "value": " 2/2 [01:24&lt;00:00, 38.39s/it]"
          }
        },
        "e9a3d8bb9fa542f98660f1bc93fc3f1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e5b6c140d47403eba82aa0afa1992db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3168e27333d435f9ccb9a3bd966887f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c4f40497c364181b9cdc61b227c75d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4e2fd34f52346fc90319b3183d0704b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "becab5a8ce0f46279577259e5e1b523e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cfd3470fd734418b393a4f2b7de5f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardik-05/Custom_Llama2/blob/training_data/Llama2_50000_10_epoch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine Tune LLama 2 on hardikch05/Text_to_sql_custom_dataset"
      ],
      "metadata": {
        "id": "Jk4r_LooEbso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Installation\n"
      ],
      "metadata": {
        "id": "OSHlAbqzDFDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GLXwJqbjtPho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a4306a-efaf-4a5f-fb06-7dbffa92d349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('huggingface')"
      ],
      "metadata": {
        "id": "tyTpclsSOczo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning Llama 2 model - LORA\n",
        "full fine-tuning, [LoRA](https://arxiv.org/abs/2106.09685), and [QLoRA](https://arxiv.org/abs/2305.14314).\n",
        "![](https://i.imgur.com/7pu5zUe.png)\n"
      ],
      "metadata": {
        "id": "AC40us6jfO_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "base_model = \"NousResearch/Llama-2-7b-hf\"\n",
        "new_model = \"llama2_test_gen_v1.0_7B\"\n",
        "\n",
        "# Dataset\n",
        "dataset = load_dataset(\"hardikch05/Text-to-sql-v1-50000\", split=\"train\")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "_iRQ0JEQ4hL6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization and LORA configs"
      ],
      "metadata": {
        "id": "c_ukG-8Uobvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "# Load base moodel\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "# Cast the layernorm in fp32, make output embedding layer require grads, add the upcasting of the lmhead to fp32\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "7204c1dbbd984a1a842dc24b4427608e",
            "6708e280be53489a8b4e2f5c7683968c",
            "b9996ecfbc584bd3a3eb30b34a3d6f12",
            "6d19d7b7d0bb455f8d80133ced67da4c",
            "e9a3d8bb9fa542f98660f1bc93fc3f1d",
            "7e5b6c140d47403eba82aa0afa1992db",
            "d3168e27333d435f9ccb9a3bd966887f",
            "0c4f40497c364181b9cdc61b227c75d4",
            "d4e2fd34f52346fc90319b3183d0704b",
            "becab5a8ce0f46279577259e5e1b523e",
            "2cfd3470fd734418b393a4f2b7de5f0d"
          ]
        },
        "id": "WZbR1IEL4g8G",
        "outputId": "e254808e-389d-465d-f295-d6174f03c9f9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7204c1dbbd984a1a842dc24b4427608e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training **args**"
      ],
      "metadata": {
        "id": "mYqYzijopUn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=10,\n",
        "        gradient_accumulation_steps=1,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=1000,\n",
        "        logging_steps=1,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        warmup_steps=10,\n",
        "        report_to=\"wandb\",\n",
        "        max_steps=2, # Remove this line for a real fine-tuning [IMPORTANT]\n",
        ")\n",
        "\n",
        "#Supervised fine-tuning\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"instruction\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "OJXpOgBFuSrc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "d9e82a79-acec-4a71-c456-160815e82811"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:08, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weights & Biases (wandb) is used track the training progress.\n",
        "\n",
        "Report - https://wandb.ai/metaalgo/huggingface/reports/Llama-2-training-fine-tuning--Vmlldzo3MTA2Mjk3"
      ],
      "metadata": {
        "id": "ngHGIdSM0Jlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run text generation pipeline with trained model\n",
        "prompt = \"Create a table (employee_name varchar(20), employeeid varchar(20), employee_address varchar(20))\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "id": "frlSLPin4IJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd260da-edd3-40aa-ac45-672b458ced8b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"employee_name\": \"John\",\n",
            "  \"employeeid\": \"123456\",\n",
            "  \"employee_address\": \"123456\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Create a table (employee_name varchar(20), employeeid varchar(20), employee_address varchar(20))\n",
            "\n",
            "### Response\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Create table employee\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKI0mJeOmTf1",
        "outputId": "2d2d3b65-30b8-40f0-b6d0-574c0493f117"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "    \"status\": \"success\",\n",
            "    \"message\": \"Table created successfully\",\n",
            "    \"data\": {\n",
            "        \"id\": 1,\n",
            "        \"name\": \"employee\",\n",
            "        \"created_at\": \"2020-05-12T10:00:00.000Z\",\n",
            "        \"updated_at\": \"2020-05-12T10:00:00.000Z\"\n",
            "   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Create table student\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dubzwJkUmTYm",
        "outputId": "aafb9e24-8678-48c0-ac28-687dd9dc7724"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "    \"status\": \"success\",\n",
            "    \"data\": {\n",
            "        \"id\": 1,\n",
            "        \"name\": \"student1\",\n",
            "        \"age\": 10\n",
            "    }\n",
            "}\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Create table student\n",
            "\n",
            "### Response:\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"status\": \"success\",\n",
            "    \"data\": {\n",
            "        \"id\": 2,\n",
            "        \"name\": \"student2\",\n",
            "        \"age\": 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Create procedure sumnumber : This procedure sums up two numbers\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jizwqjSmTKq",
        "outputId": "7d184949-ddad-43ba-b2d4-3d33ddae07b3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```\n",
            "sumnumber(1,2)\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Create procedure sumnumber : This procedure sums up two numbers\n",
            "\n",
            "### Response:\n",
            "\n",
            "```\n",
            "sumnumber(1,2)\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Create procedure sumnumber : This procedure sums up two numbers\n",
            "\n",
            "### Response:\n",
            "\n",
            "```\n",
            "sumnumber(1,2)\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"SQL command for creating table employee\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeuiZLC-mTD_",
        "outputId": "d01ea068-a045-450b-b106-5db6b8db7311"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": \"success\",\n",
            "  \"message\": \"Table created successfully\",\n",
            "  \"data\": {\n",
            "    \"id\": 1,\n",
            "    \"name\": \"employee\",\n",
            "    \"created_at\": \"2020-05-12T10:00:00.000Z\",\n",
            "    \"updated_at\": \"2020-05-12T10:00:00.000Z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"SQL command for deleting a record in table\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAVzilm_mS9l",
        "outputId": "045d706a-5f18-4114-8bdb-c9e1ec70d1c9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Example:\n",
            "\n",
            "### Request:\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Example:\n",
            "\n",
            "### Request:\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Example:\n",
            "\n",
            "### Request:\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Example:\n",
            "\n",
            "### Request:\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Example:\n",
            "\n",
            "### Request:\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Example:\n",
            "\n",
            "### Request:\n",
            "\n",
            "### Response:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"SQL command for Insert sudent info in student database\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_67jZltmSyO",
        "outputId": "94f38576-cd80-444e-c6bb-19108e59fb24"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": true,\n",
            "  \"message\": \"Student inserted successfully\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"student_id\": 1,\n",
            "  \"student_name\": \"John\",\n",
            "  \"student_age\": 20,\n",
            "  \"student_gender\": \"Male\",\n",
            "  \"student_address\": \"123456\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Instruction:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"SQL syntax to view student database\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrnqi6U-mSq1",
        "outputId": "1554884b-190a-4ef8-829d-3d20fbc941eb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"id\": 1,\n",
            "      \"name\": \"John\",\n",
            "      \"age\": 20,\n",
            "      \"address\": \"1234 Main St\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 2,\n",
            "      \"name\": \"Jane\",\n",
            "      \"age\": 21,\n",
            "      \"address\": \"5678 Elm St\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL Language\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gNO0TUumSf2",
        "outputId": "e2ee2368-fc61-4c59-ba11-4a0e3cebb158"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Example:\n",
            "\n",
            "```sql\n",
            "SELECT * FROM table_name\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```sql\n",
            "SELECT * FROM table_name WHERE column_name = 'value'\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```sql\n",
            "SELECT * FROM table_name WHERE column_name = 'value' AND column_name = 'value'\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```sql\n",
            "SELECT * FROM table_name WHERE column_name = 'value' AND column_name = '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL Language to add values in a table\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jclTkUxErJJ3",
        "outputId": "ec6c4420-86ec-4d95-997e-6ef7d38056ee"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": true,\n",
            "  \"message\": \"Successfully added values in a table\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"table_name\": \"table_name\",\n",
            "  \"values\": [\n",
            "    {\n",
            "      \"column_name\": \"column_name\",\n",
            "      \"value\": \"value\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "### Request:\n",
            "\n",
            "```json\n",
            "{\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL Language to add two numbers\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HHPW5zQrJHZ",
        "outputId": "fd8d6855-a3cc-41fb-eef8-eafd0ae82271"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": true,\n",
            "  \"message\": \"Success\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": true,\n",
            "  \"message\": \"Success\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Error:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": false,\n",
            "  \"message\": \"Error\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": false,\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL Language for viewing current database\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "953XxNf6rJE1",
        "outputId": "bc332490-4b5b-4cf5-ba3b-4e94b269e71f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": true,\n",
            "  \"message\": \"Successfully created the database\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": true,\n",
            "  \"message\": \"Successfully created the database\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Request:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"database\": \"database_name\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Response:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"status\":\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"SQL Language code for adding two numbers\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfK8O_6PrI_1",
        "outputId": "918ca0bb-5fed-4e54-90b8-382925431433"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"success\": true,\n",
            "  \"data\": {\n",
            "    \"result\": 10\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"sql\": \"SELECT 1 + 1\",\n",
            "  \"success\": true,\n",
            "  \"data\": {\n",
            "    \"result\": 2\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "SQL Language code for subtracting two numbers\n",
            "\n",
            "### Response:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL to sum two numbers\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRZ4UTZHrI02",
        "outputId": "1d6916df-450d-4933-c4dd-d7c452a1f689"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "SELECT SUM(a) + SUM(b)\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "write code in SQL to sum two numbers\n",
            "\n",
            "### Response:\n",
            "```\n",
            "SELECT SUM(a) + SUM(b)\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "write code in SQL to sum two numbers\n",
            "\n",
            "### Response:\n",
            "```\n",
            "SELECT SUM(a) + SUM(b)\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "write code in SQL to sum two numbers\n",
            "\n",
            "###\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"10 lines of sql code\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcGeAKzCsbRw",
        "outputId": "8ed72e06-6f20-42ec-a664-774b495f41fb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Example:\n",
            "\n",
            "### Input:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Explanation:\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "10 lines of sql code\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Example:\n",
            "\n",
            "### Input:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Explanation:\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "10 lines of sql code\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Example:\n",
            "\n",
            "### Input:\n",
            "\n",
            "### Output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"50 lines of sql code\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQomOep5sxm-",
        "outputId": "f244325a-c8dc-4ff0-b3b8-c0822705a884"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Example:\n",
            "\n",
            "### Input:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Explanation:\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "50 lines of sql code\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Example:\n",
            "\n",
            "### Input:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Explanation:\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "50 lines of sql code\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Example:\n",
            "\n",
            "### Input:\n",
            "\n",
            "### Output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write sql code for displaying table column name\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVY6I70ZsxZn",
        "outputId": "a8f449ea-26b2-47d7-e127-3c6b0506be7a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"column_name\": \"column_name_1\",\n",
            "      \"column_name\": \"column_name_2\",\n",
            "      \"column_name\": \"column_name_3\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"column_name\": \"column_name_1\",\n",
            "      \"column_name\":\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write SQL code to create a new employee in employeetable (employeename: 'Hardik', designation : 'Intern')\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzWTue8ks-62",
        "outputId": "b868e1e4-560d-44e2-c65c-22342fde7358"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"employee\": {\n",
            "    \"employeename\": \"Hardik\",\n",
            "    \"designation\": \"Intern\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "write SQL code to create a new employee in employeetable (employeename: 'Hardik', designation : 'Intern')\n",
            "\n",
            "### Response:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL Language to declare a function sum(num1,num2) which returns sum of two numbers\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYjFxNittWK-",
        "outputId": "57722aa1-08d2-43dc-85c8-6e254a01e5b6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "CREATE FUNCTION sum(num1 INT, num2 INT)\n",
            "RETURNS INT\n",
            "LANGUAGE SQL\n",
            "AS\n",
            "BEGIN\n",
            "    RETURN num1 + num2;\n",
            "END;\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "write code in SQL Language to declare a function sum(num1,num2) which returns sum of two numbers\n",
            "\n",
            "### Response:\n",
            "```\n",
            "CREATE FUNCTION sum(num\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write 10 lines code in SQL Language\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5ce8Y7VtnMO",
        "outputId": "7c545b47-cb3c-40e9-b581-5940f816c696"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "    \"status\": true,\n",
            "    \"message\": \"Success\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"status\": true,\n",
            "    \"message\": \"Success\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Error:\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"status\": false,\n",
            "    \"message\": \"Error\"\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"status\": false,\n",
            "   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL Language to declare a function sum(num1,num2) which returns multiplication of two numbers\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ0cYu7luvaH",
        "outputId": "5815aaac-f428-46f8-d087-08e2567e0147"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```\n",
            "CREATE FUNCTION sum(num1,num2)\n",
            "RETURNS NUMERIC\n",
            "LANGUAGE SQL\n",
            "AS\n",
            "BEGIN\n",
            "    RETURN num1*num2;\n",
            "END;\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "write code in SQL Language to declare a function sum(num1,num2) which returns multiplication of two numbers\n",
            "\n",
            "### Response:\n",
            "\n",
            "```\n",
            "CREATE FUNCTION\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL Language to declare a procedure Product(num1,num2) which returns product of two numbers\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FloDMrNau0Sg",
        "outputId": "34e8f892-6eae-41f7-e6cb-d2c2977e099a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "CREATE PROCEDURE Product(IN num1 INTEGER, IN num2 INTEGER)\n",
            "RETURNS INTEGER\n",
            "BEGIN\n",
            "    RETURN num1*num2;\n",
            "END;\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "write code in SQL Language to declare a procedure Product(num1,num2) which returns product of two numbers\n",
            "\n",
            "### Response:\n",
            "```\n",
            "CREATE PROCED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL Language to declare a procedure test which has three sql functions. First function is declared as sumnum(num1,num2) and returns product of num1 and num2. Second function is declared as prodnum(num1,num2) which returns product of two number. Third function is declared as prodsum(num1,num2) which returns the sum of product of two numbers and sum of two numbers\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt-Y337dvL_x",
        "outputId": "d2056a0e-bf7d-4348-c29b-d097439c73a9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```\n",
            "CREATE PROCEDURE test\n",
            "(\n",
            "    @num1 int,\n",
            "    @num2 int\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL Language to declare a procedure named 'test' which has three sql functions. First function is declared as sumnum(num1,num2) and returns product of num1 and num2. Second function is declared as prodnum(num1,num2) which returns product of two number. Third function is declared as prodsum(num1,num2) which returns the sum of product of two numbers and sum of two numbers\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0boIUjx9wJIH",
        "outputId": "129af3ec-c4dc-4197-b030-5fcbcbfa3134"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": \"success\",\n",
            "  \"data\": {\n",
            "    \"test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write code in SQL Language to declare a procedure named 'test' which has three sql functions.\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcK-p_ukwUMW",
        "outputId": "cbca54bd-dd4d-4bff-941c-f6c8c74260ce"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```\n",
            "CREATE PROCEDURE test\n",
            "(\n",
            "    @a int,\n",
            "    @b int,\n",
            "    @c int\n",
            ")\n",
            "AS\n",
            "BEGIN\n",
            "    SELECT @a + @b + @c;\n",
            "END\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "write code in SQL Language to declare a procedure named 'test' which has three sql functions.\n",
            "\n",
            "### Response:\n",
            "\n",
            "```\n",
            "CREATE PROCEDURE test\n",
            "(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Return a boiler template for oracle PLSQL unit test package\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZIkrob6wcqe",
        "outputId": "26c4e5d2-aab1-47c9-fc9a-a1a79f4b26d3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```json\n",
            "{\n",
            "  \"status\": \"success\",\n",
            "  \"data\": {\n",
            "    \"boiler\": {\n",
            "      \"name\": \"boiler\",\n",
            "      \"description\": \"Boiler\",\n",
            "      \"type\": \"boiler\",\n",
            "      \"unit\": \"kW\",\n",
            "      \"value\": 1000000\n",
            "    }\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"name\": \"bo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"How to write unit test for oracle PLSQL package\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI8Z7YQHwnEW",
        "outputId": "09dccea0-5895-4f19-cb1a-237c98a55370"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```\n",
            "{\n",
            "  \"status\": 200,\n",
            "  \"data\": {\n",
            "    \"result\": true,\n",
            "    \"message\": \"Successfully created the package\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "### Example:\n",
            "\n",
            "```\n",
            "curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"test_package\",\"package_type\":\"PACKAGE\"}' http://localhost:8080/api/v1/packages\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkQCviG0Zta-",
        "outputId": "7d0150f4-de93-4717-ac84-a803c4c56412"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging the base model with the trained adapter"
      ],
      "metadata": {
        "id": "_g0fB7P9s0ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        "    do_sample=False,\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "QQn30cRtAZ-P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "d754b7bb47ad4654884b926d8ed21c7f",
            "9fc7e5606e834b5db0698afa0c3ff429",
            "c451b773fed748da8a8884f0f91e06c3",
            "354e9dac67c34f379595dc3fe33995c1",
            "ec151cdae2e64aca93cd292f92810fdf",
            "3e559a573a7e40cba38ce90feb3ae269",
            "a018912e6277494faffbbecc208d5b64",
            "1d35ab40fe6c4704b310146bd0412bba",
            "ffcfadd2fa954474a37a67ab284d642b",
            "68563c45857f42bd93a7845001693cf1",
            "71afa117ae664d29b92a74866c3358b7"
          ]
        },
        "outputId": "935bfb8e-c29e-4915-8b6a-9237c66887f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d754b7bb47ad4654884b926d8ed21c7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pushing the model and tokenizer to the Hugging Face Hub."
      ],
      "metadata": {
        "id": "n4_wCHy_s--5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\n",
        "#tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)"
      ],
      "metadata": {
        "id": "x-xPb-_qB0dz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "c6dc2ff8-be8e-4062-89e1-7b8892998105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.\n\nThrown during validation:\n[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, config_file_name, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaught_warnings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2482d6ce99c0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_temp_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2499\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_memory_footprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_buffers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;31m# Save all files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_shard_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_shard_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_serialization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_serialization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;31m# Update model card if needed:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                             \u001b[0;34m\"exception in v4.41.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m                         )\n\u001b[0;32m-> 2326\u001b[0;31m                 \u001b[0mmodel_to_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_hf_peft_config_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, config_file_name, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    581\u001b[0m                 \u001b[0;34m\"The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0;34m\"Fix these issues to save the configuration.\\n\\nThrown during validation:\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.\n\nThrown during validation:\n[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dt_sVpRcs_vX"
      }
    }
  ]
}